# twitter_airflow_EC2_pipeline

<strong>Project Overview:</strong> <br>
The objective of this project is get myself familiarized with Apache Airflow. With this in mind, I have focused more on the execution sode of the project over utilizing complex logic. 
I have used the twitter API as a data source, python and tweepy to connect and extract this information, pandas to transform the data and AWS EC2 and S3 to store and process our pipeline.
Finally, we have utlized Airflow to create a workflow, understanding the basics such as - DAG's, Tasks, Operators and Executors.

<strong>Technologies Used:</strong> <br>
Amazon Web Services: EC2, S3, IAM, Apache Parquet <br>
Tools: Airflow, Pandas, Tweepy, Dataframes
Languages: Python

